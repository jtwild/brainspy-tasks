filter_finder:
    max_attempts: 10
    main_results_dir: tmp/output/filter_finder/march_single/run6_device_hans
    results_base_dir: tmp/output/filter_finder/march_single/run6_device_hans
    save_plot: true
    show_plots: false
    algorithm_configs:
        algorithm: gradient_descent
        #results_base_dir: tmp/output/filter_finder/testruns\patch_filter_4_points_2020_03_02_172058    #Not required, because algorithm is called with is_main=False flag
        hyperparameters:
            nr_epochs: 5000
            batch_size: 4
            learning_rate: 0.0005                                                         # With noise, 0.0005 seems like a good value for patch filtering.
            loss_function: sigmoid_nn_distance
            optimizer: adam
            #betas: [0.99, 0.999]                                                           # Beta values used for the Adam optimizer. When not spupplied, torch default values (0.9, 0.999) will be used.
            stop_threshold: -1000
        processor:
            architecture: single_device
            platform: simulation
            simulation_type: neural_network
            network_type: IOnet                                                           #IOnet, dnpu, ...
            IOinfo:                                                                         # Only necessary when calling an instance of IOnet
                #output_high: 76                    # Comment out to use fdefault values taking from info of the model.
                #output_low: -330                   # max and minimum info of the training daata. Outside will be regularized by adding a loss.
                mode: multi_scaler # single_scaler or multi_scaler
                alpha: 100      # the regularizer on input voltages gets multiplied by this factor, because otherwise a 0.1 offset of input voltage only gives 0.1 to the loss.
                scaling_low: [-1.2000, -1.2000, -1.0000, -1.0000]                   # Comment out to use default values of the model
                scaling_high: [0.5000, 0.5000, 0.5000, 0.3000]
                offset_low: [-0.3500, -0.3500, -0.2500, -0.3500]
                offset_high: [-0.3500, -0.3500, -0.2500, -0.3500]
            torch_model_dict: tmp/input/models/model_brains06032020.pt
            use_noise: false
            input_indices: [1,2,3,4]
            input_electrode_no: 7                                                           # The total amount of input electrodes. Required for the processor to determine the control electrodes.
            waveform:
                amplitude_lengths: 1                                                        # In simulation, you can set this to 0 to just feed one datapoint.
                slope_lengths: 0
                output_clipping_value: 3.55
        checkpoints:
            use_checkpoints: false
            save_dir: algorithm_checkpoints
            save_interval: 200
            live_plot: false
            live_plot_interval: 20
    boolean_gate_test:
        validation:
            processor:
                waveform:
                    amplitude_lengths: 1
                    slope_lengths: 0
                    output_clipping_value: 3.55
        algorithm_configs:
            algorithm: gradient_descent
            processor:
                auto_generate_inputs: true
                point_generation_sets: [[0, 1]] # the points that will be used to autogenerate hypercubes of inputs. Must be smth by 2. For example, [[-0.7, 0.3]] or [[-1, 0], [-0.5, 0.5]]
                platform: simulation
                input_indices: [1,2,3,4]
                input_electrode_no: 7
                num_elec: 8
                waveform:
                    use_waveform: true
                    amplitude_lengths: 1
                    slope_lengths: 0
                    output_clipping_value: 3.55
